---
title: "Statpadders"
author: "Toma Shigaki-Than, CJ Frederickson, Camden Reeves, Sam Kakarla"
date: "March 17, 2025"
format: pdf
execute: 
  warning: false
  message: false
  echo: false
editor: visual
---

```{r}
#| label: load packages and data

library(tidyverse)
library(tidymodels)
library(dplyr)
library(patchwork)
library(corrplot)

imdb_top_1000 <- read_csv("data/imdb_top_1000.csv") |>
  drop_na()
```

# Introduction

The proliferation of online review platforms has significantly influenced consumer decision-making across various industries, but particularly in entertainment. In the film industry, consumers rely on both professional critics and amateur audiences to gauge the quality of movies before making viewing decisions (Moon, Bergey, & Iacobucci, 2010). While professional critics evaluate films based on artistic and technical merit, amateur audiences may assess them based on personal enjoyment, accessibility, and entertainment value. This dual review system has created a dynamic where movies may receive different evaluations from critics and general audiences, raising questions about the relationship between the two and the factors influencing their divergence. Given this context, our research seeks to address the following question: What factors in a film influence IMDb user ratings and critic meta-scores; how do differences in these scores relate to movie characteristics such as gross earnings, number of votes, decade released, runtime, certificate, and genre?

**Motivation and Importance**

Understanding the discrepancies between audience and critic ratings is critical for multiple stakeholders in the film industry. For movie studios and marketers, aligning promotional strategies with audience preferences while maintaining critical appeal can be a determinant of box office success and long-term profitability (Moon, Bergey, & Iacobucci, 2010). Platforms like Netflix and IMDb utilize rating-based recommendation systems to enhance user satisfaction and engagement, making it essential to refine these systems based on nuanced insights into rating behaviors. Moreover, prior research suggests that word-of-mouth (WOM) effects can sustain movie revenues over time, with online user ratings playing a crucial role in influencing later viewership trends. Understanding these dynamics is essential for optimizing film production, marketing investments, and recommendation algorithms in an era where consumer feedback is widely accessible and highly influential.

**Hypotheses and Theoretical Considerations**

We propose several key hypotheses regarding the divergence between IMDb ratings (representing audience scores) and critic meta-scores:

**The Effect of Gross Earnings and Number of Votes**

Are movies with higher gross earnings and a greater number of votes more likely to have higher IMDb ratings due to their broad audience reach, (while critics may evaluate them more critically, leading to a potential divergence in scores)? Conversely, could lower-grossing films receive higher meta-scores due to stronger artistic value but may not resonate as widely with general audiences, leading to lower IMDb ratings?

**Impact of Decade Released and Runtime**

Could older movies have higher meta-scores due to their established reputation and critical re-evaluation over time, while IMDb ratings could fluctuate based on contemporary audience preferences? What about runtime? Could longer runtime movies receive higher meta-scores as they are often associated with more complex storytelling, while audiences may rate them lower due to attention-span and pacing concerns?

**The Role of Certificate (Censorship Rating)**

R-rated movies may receive higher IMDb ratings due to mature content attracting a dedicated audience, whereas critics may assess them more rigorously depending on the execution of themes. Family-friendly movies may receive higher meta-scores due to broader accessibility but could have lower IMDb ratings if audiences find them less engaging compared to other categories. By investigating these hypotheses using this dataset that includes IMDb ratings, critic meta-scores, and various film characteristics, this study aims to provide a comprehensive analysis of the factors influencing rating discrepancies. Ultimately, our findings will offer valuable insights into how different audience segments perceive film quality, with implications for movie marketing, recommendation algorithms, and consumer behavior in the digital entertainment landscape.

In summary the following is our primary research question: how do different variable predictors in genre, censorship, runtime, and gross earnings predict how critics and fans will rate movies comparatively?

Reference: Moon, S., Bergey, P. K., & Iacobucci, D. (2010). Dynamic Effects among Movie Ratings, Movie Revenues, and Viewer Satisfaction. Journal of Marketing, 74(1), 108-121. https://doi.org/10.1509/jmkg.74.1.108

**Potential Predictors:**
- Runtime (numerical)

- Gross Revenue (numerical)

- Certificate (categorical)

- Decade Released (categorical)

- Number of Votes (numerical)

**Response Variables**
- Meta-Score (aggregate score of critics' ratings)

- IMDB Score (score given by audience/fans on IMDB, scaled to match Meta-Score)

- Difference (The quantity that the meta-score differs from the IMDB score)

# Exploratory Data Analysis

To begin our EDA, we first had to deal with the NA values in our data. Some observations had NA values in their Gross Revenues. After examining these observations, there were no discernible patterns or connections between the NA values; they were random. As such, we were able to drop these values without compromising our data set or losing important observations. We also created the variable `difference`, whose value indicates the difference between MetaScore and IMDB Score, scaled so that they can be compared. A negative value indicates that the MetaScore is lower than IMDB Score, and a positive value indicates that it is higher.

```{r}
imdb_top_1000 <- imdb_top_1000 |>
  mutate(no_votes_scaled = No_of_Votes / 10^6,
         gross_scaled = Gross / 10^6,
         IMDB_scaled = IMDB_Rating *10,
         Released_Year = if_else(Series_Title == "Apollo 13", "1995", 
                                 Released_Year),
         difference = Meta_score - IMDB_scaled,
         runtime = as.numeric(str_remove(Runtime, " min")),
         Released_Year = as.numeric(Released_Year),
         decade = case_when(Released_Year < 1940 ~ "1930s", 
                          Released_Year >= 1940 & Released_Year < 1950 ~ "1940s",
                          Released_Year >= 1950 & Released_Year < 1960 ~ "1950s",
                          Released_Year >= 1950 & Released_Year < 1960 ~ "1950s",
                          Released_Year >= 1960 & Released_Year < 1970 ~ "1960s",
                          Released_Year >= 1970 & Released_Year < 1980 ~ "1970s",
                          Released_Year >= 1980 & Released_Year < 1990 ~ "1980s",
                          Released_Year >= 1990 & Released_Year < 2000 ~ "1990s",
                          Released_Year >= 2000 & Released_Year < 2010 ~ "2000s",
                          Released_Year >= 2010 ~ "2010s",))
```

Further, we turned our year predictor into a categorical variable by creating a new variable: `decade`. Since there is a very wide range of values in `Released_Year` for the movies selected, that variable itself is not particularly useful for our analysis. Not many observations even had the same released year, and the differences between one unit in that variable were arbitrary for some movies (for example a movie released in 1966 vs 1967 does not give much insight) For data cleaning and to improve clarity and interpretability, we changed this variable into a categorical variable `decade`, where all of the years released are grouped into decades (i.e. 1950s, 1960s, etc.) This categorical approach gives better interpretability; grouping movies into decades creates a better identifier than simply using individual years.

Additionally, the variable `Runtime` listed the runtime of each observation as a string with the number of minutes followed by the word "mins." For example, a movie 90 minutes long would be listed as the string "90 mins" instead of the number 90. As such, this made `Runtime` a categorical variable. We changed this by removing the "mins" label and refactoring it as numeric, thus making the `Runtime` into a numerical variable.

# Potential Multicollinearity
```{r}
#| label: correlation matrix

#geeksforgeeks.org/correlation-matrix-in-r-programming

matrix <- imdb_top_1000 %>% 
  select(Released_Year, runtime, IMDB_scaled, Meta_score, no_votes_scaled,
         gross_scaled, difference)
c <- cor(matrix)
corrplot(c, method = "number")
```

Here, we created a correlation matrix to see which of our numerical predictors may be highly correlated, thus indicating potential multicollinearity. There is high correlation (0.97 correlation coefficient) between the variable `difference` and the variables `IMDB_scaled` and `meta_score`, but that is to be expected - the `difference` variable is mutated from both of those variables, to show the difference between them.`IMDB_scaled` and `no_votes_scaled` are also highly correlated with a coefficient of 0.62, but that is also to be expected - of course the votes on IMDB are correlated with an IMDB score. We would not fit a model with both of those variables.

# Univariate EDA

```{r}
p1 <- imdb_top_1000 %>% 
  ggplot(aes(x = IMDB_scaled )) +
  geom_histogram() + 
  coord_cartesian(xlim = c(50, 100)) +
  labs(
    x = "Scaled IMDB Score",
    y = "Frequency"
  )
p2 <- imdb_top_1000 %>% 
  ggplot(aes(x = Meta_score )) +
  geom_histogram() +
  coord_cartesian(xlim = c(0, 100)) +
   labs(
    x = "Meta-Score",
    y = "Frequency"
  )
p3 <- imdb_top_1000 %>% 
  ggplot(aes(x = difference )) +
  geom_histogram() +
   labs(
    x = "Difference Between \nIMDB and Meta Score",
    y = "Frequency"
  )
 
p1 + p2 + p3

imdb_top_1000 %>% 
  summarise(
    mean_IMDB_Score = mean(IMDB_scaled),
    med_IMDB_Score = median(IMDB_scaled),
    sd_IMDB_Score = sd(IMDB_scaled),
    IQR_IMDB_Score = IQR(IMDB_scaled),
    min_IMDB_Score = min(IMDB_scaled),
    max_IMDB_Score = max(IMDB_scaled),
    mean_MetaScore = mean(Meta_score),
    med_MetaScore = median(Meta_score),
    sd_MetaScore = sd(Meta_score),
    IQR_MetaScore = IQR(Meta_score),
    min_MetaScore = min(Meta_score),
    max_MetaScore = max(Meta_score),
    mean_Difference = mean(difference),
    med_Difference = median(difference),
    sd_Difference = sd(difference),
    IQR_Difference = IQR(difference),
    min_Difference = min(difference),
    max_Difference = max(difference)
  ) %>% 
  pivot_longer(cols = everything(), 
               names_to = "Statistic", 
               values_to = "Value") %>%
  separate(Statistic, into = c("Measure", "Variable"), sep = "_", extra = "merge") %>%
  pivot_wider(names_from = Measure, values_from = Value)
```
As we can see from our response variables, scaled IMDB Score seems to be skewed right for these films, and scores tend to trend below between 76 and 93. Both the mean score and median score are about 79, standard deviation of about 3, IQR of 4, and a range of 17.

The distribution for meta-score seems to be skewed right, with a mean of about 77, a median about 78, a standard deviation of about 12, an IQR of 16 and a range of 72. 

Furthermore in terms of the difference between the two scores, it seems that the values are almost normally distributed, with a slight left skew. This suggests that it is equally common for a meta-score to be either higher or lower than the IMDB score, though slightly more often lower. There is an outlier of about 49 points lower than IMDB score (-49). The mean difference is about two points lower than IMDB score (-2), median of 1 point lower (-1), standard deviation of 12 points, IQR of about 15 points, and a range of 70 points.

```{r}
p1 <- imdb_top_1000 %>% 
  ggplot(aes(x = gross_scaled)) +
  geom_histogram() + labs(
    x = "Gross \n(In Millions)",
    y = "Frequency"
  )
p2 <- imdb_top_1000 %>% 
  ggplot(aes(x = runtime )) +
  geom_histogram() +
   labs(
    x = "Run Time\n(In Minutes)",
    y = "Frequency"
  )
p3 <- imdb_top_1000 %>% 
  ggplot(aes(x = no_votes_scaled)) +
  geom_histogram() +
   labs(
    x = "IMDB Votes \n(In Millions)",
    y = "Frequency"
  )
 
p1 + p2 + p3

imdb_top_1000 %>% 
  summarise(
    mean_gross = mean(gross_scaled),
    med_gross = median(gross_scaled),
    sd_gross = sd(gross_scaled),
    IQR_gross = IQR(gross_scaled),
    min_gross = min(gross_scaled),
    max_gross = max(gross_scaled),
    mean_runtime = mean(runtime),
    med_runtime = median(runtime),
    sd_runtime = sd(runtime),
    IQR_runtime = IQR(runtime),
    min_runtime = min(runtime),
    max_runtime = max(runtime),
    mean_votes = mean(no_votes_scaled),
    med_votes = median(no_votes_scaled),
    sd_votes = sd(no_votes_scaled),
    IQR_votes = IQR(no_votes_scaled),
    min_votes = min(no_votes_scaled),
    max_votes = max(no_votes_scaled)
  ) %>% 
  pivot_longer(cols = everything(), 
               names_to = "Statistic", 
               values_to = "Value") %>%
  separate(Statistic, into = c("Measure", "Variable"), sep = "_", extra = "merge") %>%
  pivot_wider(names_from = Measure, values_from = Value)

```
Exploring our numerical predictors, it seems that the distribution of gross revenue in millions of dollars seems to have a right skew, with most values being below \$500 million. It has a mean of \$78.514 million, median of \$34.850 million, standard deviation of about \$115 million, IQR of \$96.310, and a range of about \$937 million.

Run time seems fairly normal with a slight right skew. There is a potential outlier around 238 minutes. It has a mean of about 124 minutes, median of about 120 minutes, standard deviation of about 26 minutes, IQR of about 32 minutes, and a range of about 166 minutes.

Finally, number of votes has a right skew. With a potential outlier at about 2.34 million votes, it has a mean of about 356,000 votes, median of about 267,000 votes, standard deviation of about 354,000 votes, IQR of about 412,000 votes, and a range of about 2.32 million votes.

# Bivariate EDA

```{r}
library(patchwork)
p1 <- imdb_top_1000 %>% 
  ggplot(aes(x = gross_scaled, y = IMDB_scaled )) +
  geom_point() + labs(
    x = "Film Gross \n(in Millions)",
    y = "Scaled IMDB Score"
  )
p2 <- imdb_top_1000 %>% 
  ggplot(aes(x = gross_scaled, y = Meta_score )) +
  geom_point() +
   labs(
    x = "Film Gross \n(in Millions)",
    y = "Meta-Score"
  )
p3 <- imdb_top_1000 %>% 
  ggplot(aes(x = gross_scaled, y = difference )) +
  geom_point() +
   labs(
    x = "Film Gross \n(in Millions)",
    y = "Difference Between IMDB and Meta Score"
  )
 
p1 + p2 + p3


```

Add narrative here

```{r}
library(patchwork)

imdb_top_1000 %>% 
  ggplot(aes(x = decade, y = IMDB_scaled )) +
  geom_boxplot() + labs(
    title = "Scaled IMDB Score vs. Decade Released",
    x = "Decade Released",
    y = "Scaled IMDB Score"
  )


```

Judging from this initial bivariate EDA of decade released vs the scaled IMDB score, there seems to be a negative correlation between date and IMDB score; as movies are newer (coming out in more recent decades), the median scaled IMDB score tends to be lower.

```{r}
imdb_top_1000 %>% 
  ggplot(aes(x = decade, y = Meta_score )) +
  geom_boxplot() +
   labs(
    title = "MetaScore vs. Decade Released",
    x = "Decade Released",
    y = "MetaScore"
  )
```

Similarly to IMDB score, the critics' median meta-scores also seem to be lower as movies are newer. In other words, the overall aggregated critic scores for films tend to be lower for movies in more recent decades.

```{r}
imdb_top_1000 %>% 
  ggplot(aes(x = decade, y = difference )) +
  geom_boxplot() +
   labs(
    title = "Difference Between IMDB and Meta Score \nvs. Decade Released",
    x = "Decade Released",
    y = "Difference Between IMDB and Meta Score"
  )
```

Judging from this EDA, the median difference between `Meta_score` and `IMDB_scaled` also tends to be lower as movies are newer. In other words, meta-scores tend to be lower than IMDB scores in more recent decades.

```{r}
library(patchwork)
p1 <- imdb_top_1000 %>% 
  ggplot(aes(x = runtime, y = IMDB_scaled )) +
  geom_point() + labs(
    x = "Run Time \n(in Minutes)",
    y = "Scaled IMDB Score"
  )
p2 <- imdb_top_1000 %>% 
  ggplot(aes(x = runtime, y = Meta_score )) +
  geom_point() +
   labs(
    x = "Run Time \n(in Minutes)",
    y = "Meta-Score"
  )
p3 <- imdb_top_1000 %>% 
  ggplot(aes(x = runtime, y = difference )) +
  geom_point() +
   labs(
    x = "Run Time \n(in Minutes)",
    y = "Difference Between IMDB and Meta Score"
  )
 
p1 + p2 + p3

```

Add narrative here


# Interaction Effects

```{r}
imdb_top_1000 %>% 
  ggplot(aes(x = gross_scaled, y = difference, color = Certificate)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Gross Revenue vs Difference between MetaScore and IMDB Score",
       subtitle = "By Certificate",
       x = "Gross Revenue \n(In Millions)",
       y = "Difference Between MetaScore and IMDB Score")
```

```{r}
imdb_top_1000 %>% 
  ggplot(aes(x = gross_scaled, y = difference, color = decade)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Gross Revenue vs Difference between MetaScore and IMDB Score",
       subtitle = "By Decade Released",
       x = "Gross Revenue \n(In Millions)",
       y = "Difference Between MetaScore and IMDB Score",
       color = "Decade Released")
```
Here, we plotted gross revenue against the difference in scores (MetaScore minus IMDB score) to examine how these variables interact against our categorical variables. The visualizations reveal clear interaction effects in two key relationships:

Gross Revenue and Certificate

Gross Revenue and Decade Released

::: callout-important
Before you submit, make sure your code chunks are turned off with `echo: false` and there are no warnings or messages with `warning: false` and `message: false` in the YAML.
:::

```{r}

```
