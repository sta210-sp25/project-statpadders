---
title: "Analysis of Divergence Between Audience and Critic Scores"
subtitle: "For Top IMDB Films from 1950 - 2019"
author: "Toma Shigaki-Than, CJ Frederickson, Camden Reeves"
date: "March 17, 2025"
format: pdf
execute: 
  warning: false
  message: false
  echo: false
editor: visual
---

```{r}
#| label: load packages and data

library(tidyverse)
library(tidymodels)
library(dplyr)
library(patchwork)
library(corrplot)
library(openintro)
library(knitr)
library(kableExtra)  # for table embellishments
library(Stat2Data)
library(broom)
library(yardstick)
imdb_top_1000 <- read_csv("data/imdb_top_1000.csv") |>
  drop_na()
```

# Introduction

Online review platforms have transformed how audiences evaluate and choose movies. Between professional critics and everyday viewers, these platforms host a broad spectrum of opinions that increasingly shape the trajectory of films, from box office performance to streaming recommendations. However, these two groups often evaluate films through very different lenses: critics tend to emphasize artistic merit, narrative structure, and technical execution, while audiences may prioritize entertainment, emotional resonance, and accessibility. As a result, it is commonplace for a film to receive mixed signals across platforms. A film may be highly rated by audiences but poorly reviewed by critics, or vice versa.

This divergence can be confusing for consumers, who must navigate these conflicting assessments to make viewing decisions. For example, a film might thrive on word-of-mouth and accumulate high audience ratings on IMDb but perform poorly with critics or fail to receive awards recognition.

**Motivation and Importance**

Understanding these differences is essential not just for filmgoers but also for industry stakeholders. Movie studios, marketers, and streaming platforms rely on both critical acclaim and mass appeal to determine promotional strategies, content investments, and recommendation algorithms. Prior research has shown that user-driven ratings have powerful word-of-mouth effects, often extending a film’s relevance and commercial success (Moon, Bergey, Iacobucci, 2010). Further, the typical consumer does not evaluate films with the same lens or criteria as professional critics.

By analyzing these patterns, we aim to identify the film characteristics, such as decade of release, runtime, gross earnings, and censorship rating, that are most predictive of audiences liking a film more than critics. These insights have direct implications for consumer behavior research, content recommendation systems, and media marketing strategies in an increasingly data-driven entertainment landscape.

Given the observed divergence between critic and audience evaluations, we pose the following research question:

What factors contribute the differences in audience and critic scores, and how can we use these factors to predict the likelihood of significant divergence between the two?

In this study, we focus on modeling the odds that audience scores exceed critic scores by at least one standard deviation. By doing so, we shift attention from understanding average film quality to identifying conditions that foster fan-favorite films and resonate with general audiences even when they fail to win over the critics.

# Data

This data is taken from the top 1000 movies on IMDB, obtained through data scraping. In cleaning our data, we first had to deal with the NA values. Some observations had NA values in their Gross Revenues. After examining these observations, there were no discernible patterns or connections between the NA values; they were random. As such, we were able to drop these values without compromising our data set or losing important observations. We also created the variable `difference`, whose value indicates the difference between IMDB Score and MetaScore, scaled so that they can be compared. A negative value indicates that the audience score is lower than that of critics, and positive is vice versa. Because our dataset includes older and international films with various outdated or uncommon censorship ratings, we consolidated certificates into modern, widely recognizable categories: G, PG, PG-13, R, and Other, based on their closest equivalents in the current U.S. rating system.Finally, we narrowed our scope by excluding films released before the 1950s, as there were relatively few observations from that period. This is understandable given that the IMDb rating system was not introduced until the 1990s—meaning older films would have had to be retroactively added and reviewed. To account for this limitation, we removed pre-1950 entries from our dataset.

**Predictors:**

-   Runtime: *Numerical - film's duration in minutes*
-   Gross Revenue: *Numerical - film's gross revenue, in millions of dollars*
-   Censorship Certificate: *Categorical - censorship rating (e.g. PG, R, etc.)*
-   Decade Released: *Categorical - decade of film's release*
-   Number of Votes: *Numerical - number of votes film has on IMDB*

**Response Variable:**

-   Difference: *The quantity that the IMDB score (score given by audience/fans on IMDB, scaled to match MetaScore) differs from the MetaScore (aggregate score of critics' ratings)*

Anticipating the possibility of exploring logistic regression, we created a binary variable, `difference_binary`, based on the difference between IMDb audience scores and critic MetaScores. This variable identifies the differences that are significantly divergent versus those that are not. To capture meaningful divergence, we classified films into two categories:

**1 (divergent):** if the difference was less than -13 or greater than 9, corresponding approximately to films with audience-critic score differences exceeding ±1 standard deviation from the mean.

**0 (non-divergent):** otherwise.

This threshold approach is grounded in the properties of the original difference distribution, which was approximately normal with a slight right skew. With a mean of approximately -2.2 and a standard deviation of about 11.9, setting thresholds at roughly one standard deviation above and below the mean allowed us to identify films with statistically meaningful deviations rather than minor fluctuations. This method balances capturing important fan-favorite or controversial films while maintaining a reasonable sample size for modeling.

```{r}
#| label: data cleaning
imdb_top_1000 <- imdb_top_1000 |>
  mutate(no_votes_scaled = No_of_Votes / 10^6,
         gross_scaled = Gross / 10^6,
         gross_cent = scale(gross_scaled, center = TRUE, scale = FALSE),
         IMDB_scaled = IMDB_Rating *10,
         Released_Year = if_else(Series_Title == "Apollo 13", "1995", 
                                 Released_Year),
         difference = IMDB_scaled - Meta_score,
         difference_binary = factor(if_else(difference < -13 | difference > 9 , 1, 0)),
         runtime = as.numeric(str_remove(Runtime, " min")),
         Released_Year = as.numeric(Released_Year),
         decade = case_when(Released_Year < 1940 ~ "1930s", 
                          Released_Year >= 1940 & Released_Year < 1950 ~ "1940s",
                          Released_Year >= 1950 & Released_Year < 1960 ~ "1950s",
                          Released_Year >= 1950 & Released_Year < 1960 ~ "1950s",
                          Released_Year >= 1960 & Released_Year < 1970 ~ "1960s",
                          Released_Year >= 1970 & Released_Year < 1980 ~ "1970s",
                          Released_Year >= 1980 & Released_Year < 1990 ~ "1980s",
                          Released_Year >= 1990 & Released_Year < 2000 ~ "1990s",
                          Released_Year >= 2000 & Released_Year < 2010 ~ "2000s",
                          Released_Year >= 2010 ~ "2010s",),
         certificate = case_when(
      Certificate %in% c("G", "U", "APPROVED", "PASSED") ~ "G",
      Certificate %in% c("PG", "TV-PG", "GP", "UA", "U/A") ~ "PG",
      Certificate %in% c("PG-13") ~ "PG-13",
      Certificate %in% c("R", "A") ~ "R",
      TRUE ~ "Other"),
      runtime_cent = scale(runtime, center = TRUE, scale = FALSE),
      votes_cent = scale(no_votes_scaled, center = TRUE, scale = FALSE),
      
  )

imdb_top_1000 <- imdb_top_1000 %>% 
  filter(!(decade %in% c("1930s", "1940s")))
```

```{r}
#| label: response-eda
imdb_top_1000 %>% 
  ggplot(aes(x = difference )) +
  geom_histogram() +
   labs(
    x = "Difference Between \nIMDB and Meta Score",
    y = "Frequency"
  )
```

We began our study with comprehensive exploratory data analysis to assess variable relationships and potential modeling challenges. A correlation matrix identified strong correlation (0.62) between the IMDB score and number of votes a film received, leading us to avoid including both in the same model to prevent multicollinearity. Additionally, initial visualizations suggested a potential interaction effect between a film’s censorship certificate and its gross revenue. These findings informed the structure of our modeling approach. (*See Appendix for full EDA.*)

```{r}
#| label: num-predictor-eda

library(patchwork)
p1 <- imdb_top_1000 %>% 
  ggplot(aes(x = gross_scaled, y = difference )) +
  geom_point() + labs(
    x = "Film Gross \n(in Millions)",
    y = "Difference: IMDB vs Critic Scores"
  )
p2 <- imdb_top_1000 %>% 
  ggplot(aes(x = runtime, y = difference)) +
  geom_point() +
   labs(
    x = "Film Runtime \n(in Minutes)",
    y = "Difference: IMDB vs Critic Scores"
   )

p3 <- imdb_top_1000 %>% 
  
  ggplot(aes(x = no_votes_scaled, y = difference )) +
  geom_point() + labs(
    x = "Number of Votes on IMDB \n(in Millions)",
    y = "Difference Between IMDB and Critic Scores"
  )
p1 + p2 + p3 + 
  plot_annotation('Film Gross, Runtime, and IMDB Votes, vs. Score Difference')
```

When plotting our numerical predictors, there appear to be no linear relationships between each predictor and our response. (*See Appendix for full EDA.*)

# Methodology

```{r}
#| label: linear-and-log-models

imdb_linear <- lm(difference ~ gross_cent + runtime_cent + decade + certificate
                  + no_votes_scaled + gross_cent*certificate, data = 
                    imdb_top_1000)

imdb_full_log <- glm(difference_binary ~ gross_cent + runtime_cent + decade +
certificate + votes_cent + gross_cent * certificate,
data = imdb_top_1000, family = "binomial")

#imdb_linear <- lm(difference ~ gross_cent + runtime_cent + decade + certificate
     #             + no_votes_scaled + gross_cent*certificate, data = 
      #              imdb_top_1000)

#imdb_linear_no_cert <- lm(difference ~ gross_cent + runtime_cent + decade, data = 
       #             imdb_top_1000)

tidy(imdb_linear)
#tidy(imdb_full_log)

glance(imdb_linear) %>%
  select(adj.r.squared)


#glance(imdb_linear_no_cert) %>% 
 # select(adj.r.squared)
```

We initially explored a linear regression approach to model the difference between IMDb audience scores and critic MetaScores. However, diagnostics revealed no clear linear relationship between the response variable and the numerical predictors. Our Adjusted $R^2$ value of just 8.81% indicated that very little of the variation in the response was being explained by the regression model. Given this, and the bounded, binary nature of our eventual outcome (whether a film’s audience score significantly diverges from critic scores), we transitioned to a logistic regression framework.

In constructing the logistic model, we first fit a full logistic regression model including:

-   Gross revenue, centered for interpretability: `gross_cent`

-   Runtime in minutes, centered for interpretability: `runtime_cent`

-   Decade of release: `decade`

-   Number of votes, centered for interpretability: `votes_cent`

-   Censorship certificate: `certificate`

-   An interaction term between gross revenue and certificate `gross_cent` \* `certificate`

```{r}
#| label: remove certificate
#just getting rid of certificate interaction
imdb_reduced_log <- glm(difference_binary ~ gross_cent + runtime_cent + decade + certificate +
votes_cent, data = imdb_top_1000,
family = "binomial")

#getting rid of certificate altogether
imdb_log_noCert <- glm(difference_binary ~ gross_cent + runtime_cent + decade + 
votes_cent, data = imdb_top_1000,
family = "binomial")

tidy(imdb_reduced_log)


```

```{r}
#| label: drop-in-deviance
anova(imdb_reduced_log, imdb_full_log, test = "Chisq") %>% 
  tidy() %>% 
  kable(digits = 3)

#with certificate and interaction vs. no certificate
anova(imdb_log_noCert, imdb_full_log, test = "Chisq") %>% 
  tidy() %>% 
  kable(digits = 3)
```

```{r}
#| label: evaluate models
glance(imdb_full_log)
glance(imdb_log_noCert)
#glance(imdb_reduced_log)
```

Upon evaluating the model, the main effects of certificate were not statistically significant, as shown by their high p-values. However, we hypothesized from our EDA that gross revenue may interact with certificate rating (*See appendix for full EDA.*). This is to assert that, for instance, commercial performance may matter differently depending on the rating of the films.

As such, we performed a drop-in-deviance test comparing the full model (with certificate and interaction) against a reduced model excluding the interaction terms. The results showed that including the gross × certificate interaction improved model fit, though modestly. Specifically, the deviance decreased with the interaction included, suggesting that there may be some moderating effect of the certificate on gross revenue’s impact. Given this, we then narrowed our model comparison to two candidates: a model with certificate and the gross × certificate interaction terms included, and a model with no certificate variables at all (predictors: gross_cent + runtime_cent + decade + votes_cent).

To formally select between them, we compared their AIC and BIC values. Both AIC and BIC were lower for the model without certificate variables. Since BIC penalizes model complexity more heavily than AIC, and favors simpler models, it is important to note that we would expect BIC to favor the reduced model. However, the fact that both AIC and BIC favored the simpler model presented sufficient evidence that removing `certificate` and its interaction was justified. Thus, despite the modest improvement in deviance with the interaction term, the penalized model selection criteria (AIC/BIC) led us to eliminate the censorship certificate variable and its interaction terms from the final model. The final logistic regression model, used to predict whether a movie’s IMDb audience score diverged significantly from its critic MetaScore, includes centered gross revenue, centered runtime, decade of release, and centered number of votes as predictors.

# Results

```{r}
tidy(imdb_log_noCert)
```

After fitting the final logistic regression model using `gross_cent`, `runtime_cent`, `decade`, and `votes_cent` as predictors, we initially evaluated its performance using a default classification threshold of 0.5. However, under this threshold, the model overwhelmingly predicted the majority class (0 - *non-divergent*), making it largely ineffective at identifying films where audience ratings diverged significantly from critic ratings. Very few films were classified as having a major divergence, despite a meaningful portion of the dataset meeting that condition. Recognizing that our binary outcome was based on ±1 standard deviation from the mean difference (difference \< -13 or \> 9 being classified as 1), and thus rare by construction, we addressed this imbalance by lowering the classification threshold to 0.3. Under the adjusted 0.3 threshold, the model produced the following confusion matrix:

```{r}
#| label: confusion-matrix
#| 
library(yardstick)
library(dplyr)
library(ggplot2)

predictions <- imdb_top_1000 %>%
  mutate(
    .pred_1 = predict(imdb_full_log, newdata = imdb_top_1000, type = "response"),  # NAME THIS .pred_1 !!!
    pred_class = factor(if_else(.pred_1 > 0.3, "1", "0")),
    difference_binary = factor(difference_binary)
  )

# Step 2: Confusion matrix
conf_mat(predictions, truth = difference_binary, estimate = pred_class) |>
  autoplot(type = "heatmap")


# Step 3: ROC Curve Data
roc_curve_data <- roc_curve(predictions, truth = difference_binary, .pred_1)

# Step 4: Plot ROC Curve
autoplot(roc_curve_data) +
  ggtitle("ROC Curve for Predicting Audience vs Critic Rating Divergence")

# Step 5: Calculate AUC
roc_auc(predictions, truth = difference_binary, .pred_1, event_level = "second")

# More detail and classification metrics:
classification_metrics <- predictions %>%
  metrics(truth = difference_binary, estimate = pred_class) %>%
  bind_rows(
    recall(predictions, truth = difference_binary, estimate = pred_class),
    precision(predictions, truth = difference_binary, estimate = pred_class),
    specificity(predictions, truth = difference_binary, estimate = pred_class)
  )

classification_metrics

```

In evaluating model performance, we considered whether to prioritize sensitivity or specificity. Sensitivity (52.6%) measures the model’s ability to correctly identify films where audience scores diverge significantly from critic scores — the rare but important “fan-favorite” or “controversial” cases. Specificity, or recall, (66.1%) measures the ability to correctly classify typical films with no major divergence. Given that our research question focuses on understanding what factors drive major rating divergence, prioritizing sensitivity is more appropriate. Missing a fan-favorite or controversial film (a false negative) would be more detrimental to the purpose of our study than incorrectly flagging a typical film (a false positive).

Our goal is to capture as many truly divergent films as possible, even if it means accepting a slightly higher false positive rate. Lowering the classification threshold to 0.3 significantly improved the model’s sensitivity, allowing it to identify a much greater proportion of the films where audience scores diverged meaningfully from critic scores. This came at the expected cost of increasing false positives, resulting in a moderate drop in specificity and overall precision. To assess the model’s discriminative ability across all possible thresholds, we generated an ROC curve and calculated the Area Under the Curve (AUC). The resulting AUC of 0.63 suggests that the model performs just slightly better than random guessing at ranking films by their likelihood of divergence, thus still struggles. This indicates that the predictors capture some relevant patterns in the data, though their ability to sharply distinguish divergent from non-divergent films remains limited.

Among the predictors, runtime was statistically significant (p \< 0.01), with longer films being slightly more likely to exhibit audience–critic score divergence. Specifically, for every additional minute of runtime, we estimate that the odds of a film having divergent scores are multiplied on average by a factor of 1.01, holding all other variables constant. While statistically significant, this effect is relatively small in practical terms. Decade of release also showed some influence, particularly with films from the 1970s: we estimate that a 1970s film on average has 0.33 times the odds of divergence of one from the 1950s, holding all else constanr. Again, the magnitude of this effect was modest. Gross revenue and number of votes, while included in the final model, did not emerge as strong or consistent predictors. Taken together, these results suggest that while structural film characteristics can capture some variation in audience–critic divergence, they do not provide a complete picture of what drives rating divergence.

# Discussion and Conclusion

Our analysis aimed to identify film characteristics associated with significant disagreement between audience and critic evaluations. By defining divergence as a difference exceeding ±1 standard deviation between IMDb and MetaScores, we focused on the most extreme cases: films that defy critical consensus or achieve unexpected fan acclaim. The final logistic regression model found that longer runtimes and certain decades of release were modestly associated with higher odds of divergence. However, the model’s predictive performance (AUC = 0.63) reflects the limits of relying solely on structural attributes like gross revenue, runtime, decade, and number of votes. These features, while useful for identifying broad trends, do not capture the subjective or cultural factors that more directly influence how audiences and critics evaluate films. Our decision to lower the classification threshold to 0.3 was a strategic choice to prioritize sensitivity over specificity. Given the rarity of divergent films in the dataset, this adjustment allowed the model to better detect the cases central to our research question. Capturing more true divergent films—at the cost of increasing false positives—was aligned with our goal of understanding the conditions under which divergence occurs, rather than focusing only on precision. Overall, the model's limitations are understandable in the context of a rapidly evolving film industry. The relationship between critical reception and commercial success has become increasingly complex due to shifts in genre popularity, changing cultural norms, the rise of streaming platforms, and broader audience fragmentation. These are confounding variables that regression may be unable to account for. As a result, structural metadata alone is insufficient to predict the divergence in opinion between critics and general viewers. Future research would benefit from incorporating richer, content-based features such as genre, award recognition, review sentiment, and audience demographics. These variables may better capture the emotional, cultural, and narrative factors that shape divergent reception. While our model offers foundational insights, it also underscores the need for more nuanced approaches that reflect the multifaceted nature of media evaluation.

\pagebreak

# Appendix

```{r}
#| label: output final model
tidy(imdb_log_noCert)
```

To begin our EDA, we first had to deal with the NA values in our data. Some observations had NA values in their Gross Revenues. After examining these observations, there were no discernible patterns or connections between the NA values; they were random. As such, we were able to drop these values without compromising our data set or losing important observations. We also created the variable `difference`, whose value indicates the difference between MetaScore and IMDB Score, scaled so that they can be compared. A negative value indicates that the MetaScore is lower than IMDB Score, and a positive value indicates that it is higher. Further, we turned our year predictor into a categorical variable by creating a new variable: `decade`. Since there is a very wide range of values in `Released_Year` for the movies selected, that variable itself is not particularly useful for our analysis. Not many observations even had the same released year, and the differences between one unit in that variable were arbitrary for some movies (for example a movie released in 1966 vs 1967 does not give much insight). For data cleaning and to improve clarity and interpretability, we changed this variable into a categorical variable `decade`, where all of the years released are grouped into decades (i.e. 1950s, 1960s, etc.). This categorical approach gives better interpretability; grouping movies into decades creates a better identifier than simply using individual years.

Additionally, the variable `Runtime` listed the runtime of each observation as a string with the number of minutes followed by the word "mins." For example, a movie 90 minutes long would be listed as the string "90 mins" instead of the number 90. As such, this made `Runtime` a categorical variable. We changed this by removing the "mins" label and refactoring it as numeric, thus making the `Runtime` into a numerical variable.

We used a correlation matrix to check for multicollinearity. As expected, difference is highly correlated with `IMDB_scaled` and `meta_score`, since it’s derived from them. IMDB_scaled and no_votes_scaled also show strong correlation (0.62), so we avoid including both in the same model.

```{r}
#| label: correlation matrix

#geeksforgeeks.org/correlation-matrix-in-r-programming

matrix <- imdb_top_1000 %>% 
  select(Released_Year, runtime, IMDB_scaled, Meta_score, no_votes_scaled,
         gross_scaled, difference)
c <- cor(matrix)
corrplot(c, method = "number")
```

# Univariate EDA

```{r}
#| label: response eda
p1 <- imdb_top_1000 %>% 
  ggplot(aes(x = IMDB_scaled )) +
  geom_histogram() + 
  coord_cartesian(xlim = c(50, 100)) +
  labs(
    x = "Scaled IMDB Score",
    y = "Frequency"
  )
p2 <- imdb_top_1000 %>% 
  ggplot(aes(x = Meta_score )) +
  geom_histogram() +
  coord_cartesian(xlim = c(0, 100)) +
   labs(
    x = "MetaScore",
    y = "Frequency"
  )
p3 <- imdb_top_1000 %>% 
  ggplot(aes(x = difference )) +
  geom_histogram() +
   labs(
    x = "Difference Between \nIMDB and Meta Score",
    y = "Frequency"
  )
 
p1 + p2 + p3 + plot_annotation('Distribution of Potential Response Variables')

imdb_top_1000 %>% 
  summarise(
    mean_IMDB_Score = mean(IMDB_scaled),
    med_IMDB_Score = median(IMDB_scaled),
    sd_IMDB_Score = sd(IMDB_scaled),
    IQR_IMDB_Score = IQR(IMDB_scaled),
    min_IMDB_Score = min(IMDB_scaled),
    max_IMDB_Score = max(IMDB_scaled),
    mean_MetaScore = mean(Meta_score),
    med_MetaScore = median(Meta_score),
    sd_MetaScore = sd(Meta_score),
    IQR_MetaScore = IQR(Meta_score),
    min_MetaScore = min(Meta_score),
    max_MetaScore = max(Meta_score),
    mean_Difference = mean(difference),
    med_Difference = median(difference),
    sd_Difference = sd(difference),
    IQR_Difference = IQR(difference),
    min_Difference = min(difference),
    max_Difference = max(difference)
  ) %>% 
  pivot_longer(cols = everything(), 
               names_to = "Statistic", 
               values_to = "Value") %>%
  separate(Statistic, into = c("Measure", "Variable"), sep = "_", extra = "merge") %>%
  pivot_wider(names_from = Measure, values_from = Value)
```

As we can see from our response variables, scaled IMDB Score seems to be skewed right for these films, and scores tend to trend between 76 and 93. Both the mean score and median score are about 79, standard deviation of about 3, IQR of 4, and a range of 17.

The distribution for MetaScore seems to be skewed right, with a mean of about 77, a median about 78, a standard deviation of about 12, an IQR of 16 and a range of 72.

Furthermore in terms of the `difference` between the two scores, it seems that the values are almost normally distributed, with a slight left skew. This suggests that it is nearly equally common for a MetaScore to be either higher or lower than the IMDB score, though slightly more often lower. There is an outlier when MetaScore is about 49 points lower than IMDB score (-49). The mean `difference` is when about MetaScore is about two points lower than IMDB score (-2) and median at 1 point lower (-1). There is standard deviation of 12 points, IQR of about 15 points, and a range of 70 points.

```{r}
#| label: numerical eda
p1 <- imdb_top_1000 %>% 
  ggplot(aes(x = gross_scaled)) +
  geom_histogram() + labs(
    x = "Gross \n(In Millions)",
    y = "Frequency"
  )
p2 <- imdb_top_1000 %>% 
  ggplot(aes(x = runtime )) +
  geom_histogram() +
   labs(
    x = "Run Time\n(In Minutes)",
    y = "Frequency"
  )
p3 <- imdb_top_1000 %>% 
  ggplot(aes(x = no_votes_scaled)) +
  geom_histogram() +
   labs(
    x = "IMDB Votes \n(In Millions)",
    y = "Frequency"
  )
 
p1 + p2 + p3 + plot_annotation('Distribution of Key Numerical Predictors')

imdb_top_1000 %>% 
  summarise(
    mean_gross = mean(gross_scaled),
    med_gross = median(gross_scaled),
    sd_gross = sd(gross_scaled),
    IQR_gross = IQR(gross_scaled),
    min_gross = min(gross_scaled),
    max_gross = max(gross_scaled),
    mean_runtime = mean(runtime),
    med_runtime = median(runtime),
    sd_runtime = sd(runtime),
    IQR_runtime = IQR(runtime),
    min_runtime = min(runtime),
    max_runtime = max(runtime),
    mean_votes = mean(no_votes_scaled),
    med_votes = median(no_votes_scaled),
    sd_votes = sd(no_votes_scaled),
    IQR_votes = IQR(no_votes_scaled),
    min_votes = min(no_votes_scaled),
    max_votes = max(no_votes_scaled)
  ) %>% 
  pivot_longer(cols = everything(), 
               names_to = "Statistic", 
               values_to = "Value") %>%
  separate(Statistic, into = c("Measure", "Variable"), sep = "_", extra = "merge") %>%
  pivot_wider(names_from = Measure, values_from = Value)

```

Exploring our numerical predictors, it seems that the distribution of gross revenue in millions of dollars seems to have a right skew, with most values being below \$500 million. It has a mean of \$78.514 million, median of \$34.850 million, standard deviation of about \$115 million, IQR of \$96.310 million, and a range of about \$937 million.

Run time seems fairly normal with a slight right skew. There is a potential outlier around 238 minutes. It has a mean of about 124 minutes, median of about 120 minutes, standard deviation of about 26 minutes, IQR of about 32 minutes, and a range of about 166 minutes.

Finally, number of votes has a right skew. With a potential outlier at about 2.34 million votes, it has a mean of about 356,000 votes, median of about 267,000 votes, standard deviation of about 354,000 votes, IQR of about 412,000 votes, and a range of about 2.32 million votes.

```{r}
#| label: decade distribution
imdb_top_1000 %>% 
  ggplot(aes(x = decade)) +
  geom_bar() + labs(
    title = "Distribution of Decades Released",
    x = "Decade Released",
    y = "Frequency"
  )

```

The distribution of `decade` seems to be skewed left. This is expected, as newer films are more likely to have been added to the internet in real time after release whereas older films are added retroactively.

```{r}
#| label: certificate distribution
imdb_top_1000 %>% 
  ggplot(aes(x = Certificate)) +
  geom_bar() +
   labs(
     title = "Distribution of Censorship Certificates",
    x = "Certificate",
    y = "Frequency",
    fill = "Decade Released")
```

The distribution of `certificates` does not exhibit much of a normal shape, but notably the highest distribution is of "U" movies - those with unrestricted audiences.

# Bivariate EDA

```{r}
#| label: gross scaled vs responses
library(patchwork)
p1 <- imdb_top_1000 %>% 
  ggplot(aes(x = gross_scaled, y = IMDB_scaled )) +
  geom_point() + labs(
    x = "Film Gross \n(in Millions)",
    y = "Scaled IMDB Score"
  )
p2 <- imdb_top_1000 %>% 
  ggplot(aes(x = gross_scaled, y = Meta_score )) +
  geom_point() +
   labs(
    x = "Film Gross \n(in Millions)",
    y = "MetaScore"
  )
p3 <- imdb_top_1000 %>% 
  ggplot(aes(x = gross_scaled, y = difference )) +
  geom_point() +
   labs(
    x = "Film Gross \n(in Millions)",
    y = "Difference Between IMDB and Meta Score"
  )
 
p1 + p2 + p3 + 
  plot_annotation('Film Gross vs. IMDB Score, MetaScore, and Score Difference')


```

Upon initial Bivariate EDA, a clear linear relationship does not seem to appear between film's gross in millions and our three potential predictor variables. Perhaps later, to fit a model, we will need to find a variable transformation that gives us a promising model.

```{r}
#| label: IMDB vs decade
library(patchwork)

imdb_top_1000 %>% 
  ggplot(aes(x = decade, y = IMDB_scaled )) +
  geom_boxplot() + labs(
    title = "Scaled IMDB Score vs. Decade Released",
    x = "Decade Released",
    y = "Scaled IMDB Score"
  )


```

Judging from this initial bivariate EDA of decade released vs the scaled IMDB score, there seems to be a negative correlation between date and IMDB score; as movies are newer (coming out in more recent decades), the median scaled IMDB score tends to be lower.

```{r}
#| label: metaScore vs decade
imdb_top_1000 %>% 
  ggplot(aes(x = decade, y = Meta_score )) +
  geom_boxplot() +
   labs(
    title = "MetaScore vs. Decade Released",
    x = "Decade Released",
    y = "MetaScore"
  )
```

Similarly to IMDB score, the critics' median MetaScores also seem to be lower as movies are newer. In other words, the overall aggregated critic scores for films tend to be lower for movies in more recent decades.

```{r}
#| label: decade vs difference
imdb_top_1000 %>% 
  ggplot(aes(x = decade, y = difference )) +
  geom_boxplot() +
   labs(
    title = "Difference Between IMDB and Meta Score \nvs. Decade Released",
    x = "Decade Released",
    y = "Difference Between IMDB and Meta Score"
  )
```

Judging from this EDA, the median difference between `Meta_score` and `IMDB_scaled` also tends to be lower as movies are newer. In other words, MetaScores tend to be lower than IMDB scores in more recent decades.

```{r}
#| label: responses and runtime
library(patchwork)
p1 <- imdb_top_1000 %>% 
  ggplot(aes(x = runtime, y = IMDB_scaled )) +
  geom_point() + labs(
    x = "Run Time \n(in Minutes)",
    y = "Scaled IMDB Score"
  )
p2 <- imdb_top_1000 %>% 
  ggplot(aes(x = runtime, y = Meta_score )) +
  geom_point() +
   labs(
    x = "Run Time \n(in Minutes)",
    y = "MetaScore"
  )
p3 <- imdb_top_1000 %>% 
  ggplot(aes(x = runtime, y = difference )) +
  geom_point() +
   labs(
    x = "Run Time \n(in Minutes)",
    y = "Difference Between IMDB and Meta Score"
  )
 
p1 + p2 + p3 + 
  plot_annotation('Run Time vs. IMDB Score, MetaScore, and Score Difference')

```

Similar to our gross predictor, a clear linear relationship does not seem to appear between film's run time in minutes and our three potential predictor variables. Perhaps later, to fit a model, we will need to find a variable transformation here as well that gives us a promising model.

# Interaction Effects

```{r}
#| label: interaction-centered
imdb_top_1000 %>% 
  #| label: interaction
  ggplot(aes(x = gross_cent, y = difference, color = certificate)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Gross Revenue vs Difference between MetaScore \nand IMDB Score",
       subtitle = "By Certificate",
       x = "Gross Revenue \n(In Millions)",
       y = "Difference Between MetaScore and IMDB Score")
```

```{r}
#| label: interaction
imdb_top_1000 %>% 
  ggplot(aes(x = gross_scaled, y = difference, color = decade)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Gross Revenue vs Difference between MetaScore \nand IMDB Score",
       subtitle = "By Decade Released",
       x = "Gross Revenue \n(In Millions)",
       y = "Difference Between MetaScore and IMDB Score",
       color = "Decade Released")
```

Here, we plotted gross revenue against the difference in scores (MetaScore minus scaled IMDB score) to examine how these variables interact against our categorical variables. The visualizations reveal clear interaction effects in two key relationships:

Gross Revenue and Certificate

Gross Revenue and Decade Released

::: callout-important
Before you submit, make sure your code chunks are turned off with `echo: false` and there are no warnings or messages with `warning: false` and `message: false` in the YAML.
:::

# References:

Moon, S., Bergey, P. K., & Iacobucci, D. (2010). Dynamic Effects among Movie Ratings, Movie Revenues, and Viewer Satisfaction. Journal of Marketing, 74(1), 108-121. https://doi.org/10.1509/jmkg.74.1.108

ChatGPT (*https://chatgpt.com/*) was utilized to assist in computing classification metrics. Specifically, the tool was used to generate metric outputs based on a provided confusion matrix, streamlining the calculation process and reducing the potential for manual error. The results were then reviewed and filtered to include only the evaluation metrics covered in our course.

*ChatGPT*. OpenAI, https://chatgpt.com/. Accessed 25 Apr. 2025.
