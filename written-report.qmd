---
title: "Statpadders"
author: "Toma Shigaki-Than, CJ Frederickson, Camden Reeves, Sam Kakarla"
date: "March 17, 2025"
format: pdf
execute: 
  warning: false
  message: false
  echo: false
editor: visual
---

```{r}
#| label: load packages and data

library(tidyverse)
library(tidymodels)
library(dplyr)
library(patchwork)
library(corrplot)
library(openintro)
library(knitr)
library(kableExtra)  # for table embellishments
library(Stat2Data)
library(broom)
imdb_top_1000 <- read_csv("data/imdb_top_1000.csv") |>
  drop_na()
```

# Introduction

Online review platforms have transformed how audiences evaluate and choose movies. Between professional critics and everyday viewers, these platforms host a broad spectrum of opinions that increasingly shape the trajectory of films, from box office performance to streaming recommendations. However, these two groups often evaluate films through very different lenses: critics tend to emphasize artistic merit, narrative structure, and technical execution, while audiences may prioritize entertainment, emotional resonance, and accessibility. As a result, it is commonplace for a film to receive mixed signals across platforms. A film may be highly rated by audiences but poorly reviewed by critics, or vice versa.

This divergence can be confusing for consumers, who must navigate these conflicting assessments to make viewing decisions. For example, a film might thrive on word-of-mouth and accumulate high audience ratings on IMDb but perform poorly with critics or fail to receive awards recognition.

**Motivation and Importance**

Understanding these differences is essential not just for filmgoers but also for industry stakeholders. Movie studios, marketers, and streaming platforms rely on both critical acclaim and mass appeal to determine promotional strategies, content investments, and recommendation algorithms. Prior research has shown that user-driven ratings have powerful word-of-mouth effects, often extending a film’s relevance and commercial success (Moon, Bergey, Iacobucci, 2010). Further, the typical consumer does not evaluate films with the same lens or criteria as professional critics.

By analyzing these patterns, we aim to identify the film characteristics, such as decade of release, runtime, gross earnings, and censorship rating, that are most predictive of audiences liking a film more than critics. These insights have direct implications for consumer behavior research, content recommendation systems, and media marketing strategies in an increasingly data-driven entertainment landscape.

Given the observed divergence between critic and audience evaluations, we pose the following research question:

What factors contribute to audiences liking a movie more than critics, and how can we use these factors to predict the likelihood of a film performing better with audiences than critics?

In this study, we focus on modeling the odds that audience scores exceed critic scores. By doing so, we shift attention from understanding average film quality to identifying conditions that foster fan-favorite films and resonate with general audiences even when they fail to win over the critics.

# Data

This data is taken from the top 1000 movies on IMDB, obtained through data scraping. In cleaning our data, we first had to deal with the NA values in our data. Some observations had NA values in their Gross Revenues. After examining these observations, there were no discernible patterns or connections between the NA values; they were random. As such, we were able to drop these values without compromising our data set or losing important observations. We also created the variable `difference`, whose value indicates the difference between IMDB Score and MetaScore, scaled so that they can be compared. A negative value indicates that the audience score is lower than that of critics, and positive is vice versa. Because our dataset includes older and international films with various outdated or uncommon censorship ratings, we consolidated certificates into modern, widely recognizable categories: G, PG, PG-13, R, and Other, based on their closest equivalents in the current U.S. rating system.

**Predictors:**

-   Runtime (numerical)

-   Gross Revenue (numerical)

-   Censorship Certificate (categorical)

-   Decade Released (categorical)

-   Number of Votes (numerical)

**Response Variables**:

-   Difference: *The quantity that the IMDB score (score given by audience/fans on IMDB, scaled to match MetaScore) differs from the MetaScore (aggregate score of critics' ratings)*

```{r}
imdb_top_1000 <- imdb_top_1000 |>
  mutate(no_votes_scaled = No_of_Votes / 10^6,
         gross_scaled = Gross / 10^6,
         gross_cent = scale(gross_scaled, center = TRUE, scale = FALSE),
         IMDB_scaled = IMDB_Rating *10,
         Released_Year = if_else(Series_Title == "Apollo 13", "1995", 
                                 Released_Year),
         difference = IMDB_scaled - Meta_score,
         difference_binary = factor(if_else(difference > 0, 1, 0)),
         runtime = as.numeric(str_remove(Runtime, " min")),
         Released_Year = as.numeric(Released_Year),
         decade = case_when(Released_Year < 1940 ~ "1930s", 
                          Released_Year >= 1940 & Released_Year < 1950 ~ "1940s",
                          Released_Year >= 1950 & Released_Year < 1960 ~ "1950s",
                          Released_Year >= 1950 & Released_Year < 1960 ~ "1950s",
                          Released_Year >= 1960 & Released_Year < 1970 ~ "1960s",
                          Released_Year >= 1970 & Released_Year < 1980 ~ "1970s",
                          Released_Year >= 1980 & Released_Year < 1990 ~ "1980s",
                          Released_Year >= 1990 & Released_Year < 2000 ~ "1990s",
                          Released_Year >= 2000 & Released_Year < 2010 ~ "2000s",
                          Released_Year >= 2010 ~ "2010s",),
         certificate = case_when(
      Certificate %in% c("G", "U", "APPROVED", "PASSED") ~ "G",
      Certificate %in% c("PG", "TV-PG", "GP", "UA", "U/A") ~ "PG",
      Certificate %in% c("PG-13") ~ "PG-13",
      Certificate %in% c("R", "A") ~ "R",
      TRUE ~ "Other"),
      runtime_cent = scale(runtime, center = TRUE, scale = FALSE),
      votes_cent = scale(no_votes_scaled, center = TRUE, scale = FALSE)
  )
```

```{r}
#| label: correlation matrix

#geeksforgeeks.org/correlation-matrix-in-r-programming

matrix <- imdb_top_1000 %>% 
  select(Released_Year, runtime, IMDB_scaled, Meta_score, no_votes_scaled,
         gross_scaled, difference)
c <- cor(matrix)
corrplot(c, method = "number")
```

We used a correlation matrix to check for multicollinearity. As expected, difference is highly correlated with `IMDB_scaled` and `meta_score`, since it’s derived from them. IMDB_scaled and no_votes_scaled also show strong correlation (0.62), so we avoid including both in the same model.

```{r}
#| label: response-eda
imdb_top_1000 %>% 
  ggplot(aes(x = difference )) +
  geom_histogram() +
   labs(
    x = "Difference Between \nIMDB and Meta Score",
    y = "Frequency"
  )
```

```{r}
#| label: num-predictor-eda

library(patchwork)
p1 <- imdb_top_1000 %>% 
  ggplot(aes(x = gross_scaled, y = difference )) +
  geom_point() + labs(
    x = "Film Gross \n(in Millions)",
    y = "Difference Between IMDB and Critic Scores"
  )
p2 <- imdb_top_1000 %>% 
  ggplot(aes(x = runtime, y = difference)) +
  geom_point() +
   labs(
    x = "Film Runtime \n(in Minutes)",
    y = "Difference Between IMDB and Critic Scores"
   )

p3 <- imdb_top_1000 %>% 
  
  ggplot(aes(x = no_votes_scaled, y = difference )) +
  geom_point() + labs(
    x = "Number of Votes on IMDB \n(in Millions)",
    y = "Difference Between IMDB and Critic Scores"
  )
p1 + p2 + p3 + 
  plot_annotation('Film Gross, Runtime, and IMDB Votes, vs. Score Difference')
```

In terms of the `difference` between the two scores, it seems that the values are almost normally distributed, with a slight left skew. This suggests that it is nearly equally common for a IMDB Score to be either higher or lower than the MetaScore score, though slightly more often lower.

Additionally, when plotting our numerical predictors, there appear to be no linear relationships between each predictor and our response.

# Methodology

```{r}
#| label: models

imdb_full_log <- glm(difference_binary ~ gross_cent + runtime_cent + decade +
certificate + no_votes_scaled + gross_cent * certificate,
data = imdb_top_1000, family = "binomial")

tidy(imdb_full_log)
```

We selected a logistic regression model for two primary reasons. First, none of the numerical predictors exhibited a clear linear relationship with the response variable difference. Second, our goal was to model the odds that audience scores exceed those of critics. In our initial model, most levels of the certificate variable were not statistically significant. Additionally, although we hypothesized an interaction effect between gross revenue and censorship rating, the corresponding interaction terms were not significant and thus were excluded from the next model tested.

```{r}
imdb_reduced_log <- glm(difference_binary ~ gross_cent + runtime_cent + decade +
votes_cent, data = imdb_top_1000,
family = "binomial")
```

```{r}
anova(imdb_reduced_log, imdb_full_log, test = "Chisq") %>% 
  tidy() %>% 
  kable(digits = 3)
```

A drop-in-deviance test comparing a reduced model (excluding certificate and interaction terms) to a full model (including certificate and the gross × certificate interaction) yielded a statistically significant improvement in fit ($\chi^2$ = 19.648, df = 8, p = 0.012). This suggests that certificate and its interaction with gross earnings contribute meaningfully to predicting whether audience ratings exceed those of critics. Additionally, the full model’s AIC is lower, which indicates a better balance between model complexity and goodness of fit.

```{r}
glance(imdb_full_log)
glance(imdb_reduced_log)
```

# Results


```{r}
tidy(imdb_full_log) %>% 
  kable(digits = 3)
```

To address our research question, the final model we fit is this full logistic regression model predicting the odds that a movie’s IMDb audience score exceeds its critic MetaScore. The final model included centered and scaled versions of gross revenue, runtime, and number of IMDB votes, as well as release decade, certificate rating, and an interaction term between gross revenue and certificate. A drop-in-deviance test confirmed that including certificate and its interaction with gross significantly improved model fit ($\chi^2$ = 19.648, df = 8, p = 0.012). The full model also had a lower AIC than the reduced model, indicating a more favorable balance between model complexity and fit.

Initial diagnostic checks did not reveal violations of key assumptions for logistic regression. No strong outliers or high-leverage points unduly influenced the model, and multicollinearity among predictors was low aside from expected correlations between variables involved in the interaction term.

Interpretation of model coefficients reveals several noteworthy trends. For a baseline film from the 1930s with a G rating and average values for gross, runtime, and number of votes, the predicted odds that audience ratings are higher than critic ratings is, on average, approximately 0.024, or 2.4%.

Runtime was not a statistically significant predictor (p = 0.586), though the odds ratio per unit increase is 1.002, suggesting a minimal practical effect. With every one minute increase in film runtime, the model estimates that the odds of higher audience ratings on IMDB multiply by 1.002 on average, holding all else constant.

In contrast, gross revenue had a negative effect, which was significant (p = 0.010). This suggests that for G-rated films, higher gross earnings are associated with lower odds of audiences rating a movie more favorably than critics. For every \$1 million increase in gross revenue, the model estimates on average that the odds of higher audience scores is multiplied by 0.996, or 99.6%. It is important to note that while it is a decrease, it is not a huge decrease, which is an interesting relationship.

Release decade emerged as a strong predictor. Compared to films from the 1930s, those released from the 1970s through the 2010s had significantly higher odds of being rated better by audiences than critics, with odds increasing most notably in the 1990s and 2000s.

Although the main effects of certificate categories were not significant, the interaction terms reveal that the relationship between gross revenue and audience-vs-critic rating divergence depends on a film’s rating. Specifically, for PG-rated and R-rated films, higher gross revenue is associated with a significantly greater chance that audiences prefer the film over critics (p = 0.004 and p = 0.006, respectively). This interaction suggests that commercially successful PG or R films are more likely to resonate with audiences than with critics.

In summary, the model suggests that release decade, gross revenue, and its interaction with rating certificate are predictors in understanding when audiences are more favorable toward a film than critics. These results help contextualize the divergence in critical vs. popular reception and offer insights into the characteristics of films that perform better with general viewers.

\pagebreak

# Appendix

To begin our EDA, we first had to deal with the NA values in our data. Some observations had NA values in their Gross Revenues. After examining these observations, there were no discernible patterns or connections between the NA values; they were random. As such, we were able to drop these values without compromising our data set or losing important observations. We also created the variable `difference`, whose value indicates the difference between MetaScore and IMDB Score, scaled so that they can be compared. A negative value indicates that the MetaScore is lower than IMDB Score, and a positive value indicates that it is higher. Further, we turned our year predictor into a categorical variable by creating a new variable: `decade`. Since there is a very wide range of values in `Released_Year` for the movies selected, that variable itself is not particularly useful for our analysis. Not many observations even had the same released year, and the differences between one unit in that variable were arbitrary for some movies (for example a movie released in 1966 vs 1967 does not give much insight). For data cleaning and to improve clarity and interpretability, we changed this variable into a categorical variable `decade`, where all of the years released are grouped into decades (i.e. 1950s, 1960s, etc.). This categorical approach gives better interpretability; grouping movies into decades creates a better identifier than simply using individual years.

Additionally, the variable `Runtime` listed the runtime of each observation as a string with the number of minutes followed by the word "mins." For example, a movie 90 minutes long would be listed as the string "90 mins" instead of the number 90. As such, this made `Runtime` a categorical variable. We changed this by removing the "mins" label and refactoring it as numeric, thus making the `Runtime` into a numerical variable.

# Univariate EDA

```{r}
p1 <- imdb_top_1000 %>% 
  ggplot(aes(x = IMDB_scaled )) +
  geom_histogram() + 
  coord_cartesian(xlim = c(50, 100)) +
  labs(
    x = "Scaled IMDB Score",
    y = "Frequency"
  )
p2 <- imdb_top_1000 %>% 
  ggplot(aes(x = Meta_score )) +
  geom_histogram() +
  coord_cartesian(xlim = c(0, 100)) +
   labs(
    x = "MetaScore",
    y = "Frequency"
  )
p3 <- imdb_top_1000 %>% 
  ggplot(aes(x = difference )) +
  geom_histogram() +
   labs(
    x = "Difference Between \nIMDB and Meta Score",
    y = "Frequency"
  )
 
p1 + p2 + p3 + plot_annotation('Distribution of Potential Response Variables')

imdb_top_1000 %>% 
  summarise(
    mean_IMDB_Score = mean(IMDB_scaled),
    med_IMDB_Score = median(IMDB_scaled),
    sd_IMDB_Score = sd(IMDB_scaled),
    IQR_IMDB_Score = IQR(IMDB_scaled),
    min_IMDB_Score = min(IMDB_scaled),
    max_IMDB_Score = max(IMDB_scaled),
    mean_MetaScore = mean(Meta_score),
    med_MetaScore = median(Meta_score),
    sd_MetaScore = sd(Meta_score),
    IQR_MetaScore = IQR(Meta_score),
    min_MetaScore = min(Meta_score),
    max_MetaScore = max(Meta_score),
    mean_Difference = mean(difference),
    med_Difference = median(difference),
    sd_Difference = sd(difference),
    IQR_Difference = IQR(difference),
    min_Difference = min(difference),
    max_Difference = max(difference)
  ) %>% 
  pivot_longer(cols = everything(), 
               names_to = "Statistic", 
               values_to = "Value") %>%
  separate(Statistic, into = c("Measure", "Variable"), sep = "_", extra = "merge") %>%
  pivot_wider(names_from = Measure, values_from = Value)
```

As we can see from our response variables, scaled IMDB Score seems to be skewed right for these films, and scores tend to trend between 76 and 93. Both the mean score and median score are about 79, standard deviation of about 3, IQR of 4, and a range of 17.

The distribution for MetaScore seems to be skewed right, with a mean of about 77, a median about 78, a standard deviation of about 12, an IQR of 16 and a range of 72.

Furthermore in terms of the `difference` between the two scores, it seems that the values are almost normally distributed, with a slight left skew. This suggests that it is nearly equally common for a MetaScore to be either higher or lower than the IMDB score, though slightly more often lower. There is an outlier when MetaScore is about 49 points lower than IMDB score (-49). The mean `difference` is when about MetaScore is about two points lower than IMDB score (-2) and median at 1 point lower (-1). There is standard deviation of 12 points, IQR of about 15 points, and a range of 70 points.

```{r}
p1 <- imdb_top_1000 %>% 
  ggplot(aes(x = gross_scaled)) +
  geom_histogram() + labs(
    x = "Gross \n(In Millions)",
    y = "Frequency"
  )
p2 <- imdb_top_1000 %>% 
  ggplot(aes(x = runtime )) +
  geom_histogram() +
   labs(
    x = "Run Time\n(In Minutes)",
    y = "Frequency"
  )
p3 <- imdb_top_1000 %>% 
  ggplot(aes(x = no_votes_scaled)) +
  geom_histogram() +
   labs(
    x = "IMDB Votes \n(In Millions)",
    y = "Frequency"
  )
 
p1 + p2 + p3 + plot_annotation('Distribution of Key Numerical Predictors')

imdb_top_1000 %>% 
  summarise(
    mean_gross = mean(gross_scaled),
    med_gross = median(gross_scaled),
    sd_gross = sd(gross_scaled),
    IQR_gross = IQR(gross_scaled),
    min_gross = min(gross_scaled),
    max_gross = max(gross_scaled),
    mean_runtime = mean(runtime),
    med_runtime = median(runtime),
    sd_runtime = sd(runtime),
    IQR_runtime = IQR(runtime),
    min_runtime = min(runtime),
    max_runtime = max(runtime),
    mean_votes = mean(no_votes_scaled),
    med_votes = median(no_votes_scaled),
    sd_votes = sd(no_votes_scaled),
    IQR_votes = IQR(no_votes_scaled),
    min_votes = min(no_votes_scaled),
    max_votes = max(no_votes_scaled)
  ) %>% 
  pivot_longer(cols = everything(), 
               names_to = "Statistic", 
               values_to = "Value") %>%
  separate(Statistic, into = c("Measure", "Variable"), sep = "_", extra = "merge") %>%
  pivot_wider(names_from = Measure, values_from = Value)

```

Exploring our numerical predictors, it seems that the distribution of gross revenue in millions of dollars seems to have a right skew, with most values being below \$500 million. It has a mean of \$78.514 million, median of \$34.850 million, standard deviation of about \$115 million, IQR of \$96.310 million, and a range of about \$937 million.

Run time seems fairly normal with a slight right skew. There is a potential outlier around 238 minutes. It has a mean of about 124 minutes, median of about 120 minutes, standard deviation of about 26 minutes, IQR of about 32 minutes, and a range of about 166 minutes.

Finally, number of votes has a right skew. With a potential outlier at about 2.34 million votes, it has a mean of about 356,000 votes, median of about 267,000 votes, standard deviation of about 354,000 votes, IQR of about 412,000 votes, and a range of about 2.32 million votes.

```{r}
imdb_top_1000 %>% 
  ggplot(aes(x = decade)) +
  geom_bar() + labs(
    title = "Distribution of Decades Released",
    x = "Decade Released",
    y = "Frequency"
  )

```

The distribution of `decade` seems to be skewed left. This is expected, as newer films are more likely to have been added to the internet in real time after release whereas older films are added retroactively.

```{r}
imdb_top_1000 %>% 
  ggplot(aes(x = Certificate)) +
  geom_bar() +
   labs(
     title = "Distribution of Censorship Certificates",
    x = "Certificate",
    y = "Frequency",
    fill = "Decade Released")
```

The distribution of `certificates` does not exhibit much of a normal shape, but notably the highest distribution is of "U" movies - those with unrestricted audiences.

# Bivariate EDA

```{r}
library(patchwork)
p1 <- imdb_top_1000 %>% 
  ggplot(aes(x = gross_scaled, y = IMDB_scaled )) +
  geom_point() + labs(
    x = "Film Gross \n(in Millions)",
    y = "Scaled IMDB Score"
  )
p2 <- imdb_top_1000 %>% 
  ggplot(aes(x = gross_scaled, y = Meta_score )) +
  geom_point() +
   labs(
    x = "Film Gross \n(in Millions)",
    y = "MetaScore"
  )
p3 <- imdb_top_1000 %>% 
  ggplot(aes(x = gross_scaled, y = difference )) +
  geom_point() +
   labs(
    x = "Film Gross \n(in Millions)",
    y = "Difference Between IMDB and Meta Score"
  )
 
p1 + p2 + p3 + 
  plot_annotation('Film Gross vs. IMDB Score, MetaScore, and Score Difference')


```

Upon initial Bivariate EDA, a clear linear relationship does not seem to appear between film's gross in millions and our three potential predictor variables. Perhaps later, to fit a model, we will need to find a variable transformation that gives us a promising model.

```{r}
library(patchwork)

imdb_top_1000 %>% 
  ggplot(aes(x = decade, y = IMDB_scaled )) +
  geom_boxplot() + labs(
    title = "Scaled IMDB Score vs. Decade Released",
    x = "Decade Released",
    y = "Scaled IMDB Score"
  )


```

Judging from this initial bivariate EDA of decade released vs the scaled IMDB score, there seems to be a negative correlation between date and IMDB score; as movies are newer (coming out in more recent decades), the median scaled IMDB score tends to be lower.

```{r}
imdb_top_1000 %>% 
  ggplot(aes(x = decade, y = Meta_score )) +
  geom_boxplot() +
   labs(
    title = "MetaScore vs. Decade Released",
    x = "Decade Released",
    y = "MetaScore"
  )
```

Similarly to IMDB score, the critics' median MetaScores also seem to be lower as movies are newer. In other words, the overall aggregated critic scores for films tend to be lower for movies in more recent decades.

```{r}
imdb_top_1000 %>% 
  ggplot(aes(x = decade, y = difference )) +
  geom_boxplot() +
   labs(
    title = "Difference Between IMDB and Meta Score \nvs. Decade Released",
    x = "Decade Released",
    y = "Difference Between IMDB and Meta Score"
  )
```

Judging from this EDA, the median difference between `Meta_score` and `IMDB_scaled` also tends to be lower as movies are newer. In other words, MetaScores tend to be lower than IMDB scores in more recent decades.

```{r}
library(patchwork)
p1 <- imdb_top_1000 %>% 
  ggplot(aes(x = runtime, y = IMDB_scaled )) +
  geom_point() + labs(
    x = "Run Time \n(in Minutes)",
    y = "Scaled IMDB Score"
  )
p2 <- imdb_top_1000 %>% 
  ggplot(aes(x = runtime, y = Meta_score )) +
  geom_point() +
   labs(
    x = "Run Time \n(in Minutes)",
    y = "MetaScore"
  )
p3 <- imdb_top_1000 %>% 
  ggplot(aes(x = runtime, y = difference )) +
  geom_point() +
   labs(
    x = "Run Time \n(in Minutes)",
    y = "Difference Between IMDB and Meta Score"
  )
 
p1 + p2 + p3 + 
  plot_annotation('Run Time vs. IMDB Score, MetaScore, and Score Difference')

```

Similar to our gross predictor, a clear linear relationship does not seem to appear between film's run time in minutes and our three potential predictor variables. Perhaps later, to fit a model, we will need to find a variable transformation here as well that gives us a promising model.

# Interaction Effects

```{r}
imdb_top_1000 %>% 
  ggplot(aes(x = gross_cent, y = difference, color = certificate)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Gross Revenue vs Difference between MetaScore \nand IMDB Score",
       subtitle = "By Certificate",
       x = "Gross Revenue \n(In Millions)",
       y = "Difference Between MetaScore and IMDB Score")
```

```{r}
#| label: interaction
imdb_top_1000 %>% 
  ggplot(aes(x = gross_scaled, y = difference, color = decade)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Gross Revenue vs Difference between MetaScore \nand IMDB Score",
       subtitle = "By Decade Released",
       x = "Gross Revenue \n(In Millions)",
       y = "Difference Between MetaScore and IMDB Score",
       color = "Decade Released")
```

Here, we plotted gross revenue against the difference in scores (MetaScore minus scaled IMDB score) to examine how these variables interact against our categorical variables. The visualizations reveal clear interaction effects in two key relationships:

Gross Revenue and Certificate

Gross Revenue and Decade Released

::: callout-important
Before you submit, make sure your code chunks are turned off with `echo: false` and there are no warnings or messages with `warning: false` and `message: false` in the YAML.
:::

Reference: Moon, S., Bergey, P. K., & Iacobucci, D. (2010). Dynamic Effects among Movie Ratings, Movie Revenues, and Viewer Satisfaction. Journal of Marketing, 74(1), 108-121. https://doi.org/10.1509/jmkg.74.1.108
